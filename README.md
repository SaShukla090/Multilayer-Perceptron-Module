# Multilayer-Perceptron-Module
<img alt = "tanh" width = 154 scr = "https://github.com/SaShukla090/Multilayer-Perceptron-Module/blob/master/tanh.jpg">
## Neural Network implementation from scratch
### File "DL_Assignment1_19301.ipynb" has example implementation of Simple ANN with Adam opimizer.
### Currently module supports

 - Adam opimizer with batch size 1
 - Fully connected layers of any size
 - Activation layer with 
    - Tanh
    - Relu
  
 - Softmax Activation layer.
 - loss functions:
    - cross entropy loss
    
 more options for optimizers and activation functions will be added soon.
 if want specific custom optimizer or custom activation fucntion to be added to the module, you can mail me sa.shukla090@gmail.com
 
 
 Contact:
 
  - email: sa.shukla090@gmail.com
  - mobile: +91-8128290671
